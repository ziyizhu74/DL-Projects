{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of autoencoder_proj1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4mRyqytAw8N",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "the dataset can be found under /content/autoencoder/data/ml-1m"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7miOF7BSAobx",
        "colab_type": "code",
        "outputId": "23dd418a-458c-4cda-d519-bdada424c80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/tonylaioffer/autoencoder.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'autoencoder'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 174 (delta 0), reused 1 (delta 0), pack-reused 168\u001b[K\n",
            "Receiving objects: 100% (174/174), 17.58 MiB | 14.71 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiDukf2sBoZQ",
        "colab_type": "text"
      },
      "source": [
        "## Define data process methods\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y62Iea2bB5hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "def _get_training_data(FLAGS):  \n",
        "    ''' Buildind the input pipeline for training and inference using TFRecords files.\n",
        "    @return data only for the training\n",
        "    @return data for the inference\n",
        "    '''\n",
        "    '''\n",
        "    I guess here it's not read the entire dataset into memory in one time, and this is\n",
        "    a lazy operation, need session to activate it, so in this operation, it rules certain\n",
        "    actions in series:\n",
        "    -create TFRecordDataset to read files\n",
        "    -map this binary TFRecord dataset to contains feature 'movie_ratings'\n",
        "    -shuffle it to randomly extract 500 in buffer each time\n",
        "    -repeat this action infinite times ( i guess it's would end while all data are processed)\n",
        "    -from buffer get a batch of data\n",
        "    -prefetch one datapoint from the batch each time in order to iterative process data\n",
        "    all above are actions, thus we can see in train stage, it initilize a iterator \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    filenames = [os.path.join(FLAGS['tf_records_train_path'], f) for f in os.listdir(FLAGS['tf_records_train_path'])]\n",
        "    \n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    #Creates a TFRecordDataset to read one or more TFRecord files.\n",
        "    dataset = dataset.map(parse)\n",
        "    #Maps map_func across the elements of this dataset.\n",
        "    #This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements,\n",
        "    #in the same order as they appeared in the input\n",
        "    dataset = dataset.shuffle(buffer_size=500) #Randomly shuffles a tensor along its first dimension.\n",
        "                                               #buffer_size representing the number of elements from this dataset from which the new dataset will sample.\n",
        "    dataset = dataset.repeat()\n",
        "    # why repeat?\n",
        "    dataset = dataset.batch(FLAGS['batch_size'])\n",
        "    #Combines consecutive elements of this dataset into batches.\n",
        "    dataset = dataset.prefetch(buffer_size=1)\n",
        "    #Creates a Dataset that prefetches elements from this dataset.\n",
        "    \n",
        "    '''\n",
        "    dataset 2 is used to validation, here called infer\n",
        "    shuffle with buffer size 1 and batch with size 1 is because for validation, we only need one datapoint each time\n",
        "    to get corresponding prediction\n",
        "    but for train, we use batch train to speed up\n",
        "    '''\n",
        "    dataset2 = tf.data.TFRecordDataset(filenames)\n",
        "    dataset2 = dataset2.map(parse)\n",
        "    dataset2 = dataset2.shuffle(buffer_size=1)\n",
        "    # why dataset2? why buffer_size is 1 here?\n",
        "    dataset2 = dataset2.repeat()\n",
        "    dataset2 = dataset2.batch(1)\n",
        "    # why batch size is 1?\n",
        "    dataset2 = dataset2.prefetch(buffer_size=1)\n",
        "\n",
        "    return dataset, dataset2\n",
        "\n",
        "\n",
        "def _get_test_data(FLAGS):\n",
        "    ''' Buildind the input pipeline for test data.'''\n",
        "\n",
        "    filenames = [os.path.join(FLAGS['tf_records_test_path'], f) for f in os.listdir(FLAGS['tf_records_test_path'])]\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(parse)\n",
        "    dataset = dataset.shuffle(buffer_size=1)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(1)\n",
        "    dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def parse(serialized):\n",
        "    ''' Parser for the TFRecords file.'''\n",
        "\n",
        "    features = {'movie_ratings':tf.FixedLenFeature([3952], tf.float32),  \n",
        "              } # the shape is 3952? what's that mean? ! the movie_ID range between 1 and 3952\n",
        "    parsed_example = tf.parse_single_example(serialized,\n",
        "                                           features=features,\n",
        "                                           )\n",
        "    movie_ratings = tf.cast(parsed_example['movie_ratings'], tf.float32)\n",
        "    \n",
        "    return movie_ratings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7CRV052qCZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filenames = [os.path.join(FLAGS['tf_records_train_path'], f) for f in os.listdir(FLAGS['tf_records_train_path'])]\n",
        "    \n",
        "#dataset = tf.data.TFRecordDataset(filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BJJbExzqF6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset.map(parse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDHZL9hXBIjF",
        "colab_type": "text"
      },
      "source": [
        "## Define autoencoder architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkoUNR4_Gu2p",
        "colab_type": "text"
      },
      "source": [
        "### 1. Sigmoid Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iMfWX6RA1YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import model_helper\n",
        "\n",
        "\n",
        "def _get_bias_initializer():\n",
        "    return tf.zeros_initializer()\n",
        "\n",
        "\n",
        "def _get_weight_initializer():\n",
        "    return tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
        "\n",
        "\n",
        "class DAE: #Data Acquisition Engine\n",
        "    \n",
        "    def __init__(self, FLAGS):\n",
        "        ''' Implementation of deep autoencoder class.'''\n",
        "        \n",
        "        self.FLAGS = FLAGS\n",
        "        self.weight_initializer = _get_weight_initializer()\n",
        "        self.bias_initializer = _get_bias_initializer()\n",
        "        self.init_parameters()\n",
        "        \n",
        "\n",
        "    def init_parameters(self):\n",
        "        '''Initialize networks weights and biasis.'''\n",
        "        \n",
        "        with tf.name_scope('weights'):\n",
        "          #This context manager validates that the given values are from the same graph,  \n",
        "          #makes that graph the default graph, and pushes a name scope in that graph\n",
        "            self.W_1 = tf.get_variable(name='weight_1', shape=(self.FLAGS['num_v'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer) #Gets an existing variable with these parameters or create a new one\n",
        "            self.W_2 = tf.get_variable(name='weight_2', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_3 = tf.get_variable(name='weight_3', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_4 = tf.get_variable(name='weight_4', shape=(self.FLAGS['num_h'], self.FLAGS['num_v']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "        \n",
        "        with tf.name_scope('biases'):\n",
        "            self.b1 = tf.get_variable(name='bias_1', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b2 = tf.get_variable(name='bias_2', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b3 = tf.get_variable(name='bias_3', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "        \n",
        "    def _inference(self, x):\n",
        "        ''' Making one forward pass. Predicting the networks outputs.\n",
        "        @param x: input ratings\n",
        "        \n",
        "        @return : networks predictions\n",
        "        '''\\\n",
        "        \n",
        "        with tf.name_scope('inference'):\n",
        "             a1 = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(x, self.W_1),self.b1)) # sign(W1T*X+b1)\n",
        "             a2 = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(a1, self.W_2),self.b2))\n",
        "             a3 = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(a2, self.W_3),self.b3))\n",
        "             a4 = tf.matmul(a3, self.W_4)\n",
        "        return a4\n",
        "    \n",
        "    def _compute_loss(self, predictions, labels, num_labels):\n",
        "        ''' Computing the Mean Squared Error loss between the input and output of the network.\n",
        "            \n",
        "          @param predictions: predictions of the stacked autoencoder\n",
        "          @param labels: input values of the stacked autoencoder which serve as labels at the same time\n",
        "          @param num_labels: number of labels !=0 in the data set to compute the mean\n",
        "            \n",
        "          @return mean squared error loss tf-operation\n",
        "          '''\n",
        "            \n",
        "        with tf.name_scope('loss'):\n",
        "            loss_op = tf.div(tf.reduce_sum(tf.square(tf.subtract(predictions,labels))),num_labels)\n",
        "            return loss_op\n",
        "          \n",
        "        \n",
        "\n",
        "    def _optimizer(self, x):\n",
        "        '''Optimization of the network parameter through stochastic gradient descent.\n",
        "            \n",
        "            @param x: input values for the stacked autoencoder.\n",
        "            \n",
        "            @return: tensorflow training operation\n",
        "            @return: ROOT!! mean squared error\n",
        "        '''\n",
        "        \n",
        "        outputs = self._inference(x)\n",
        "        # ? the mask is same as x ?\n",
        "        mask = tf.where(tf.equal(x,0.0), tf.zeros_like(x), x) # indices of 0 values in the training set\n",
        "        # tf.zero_like : Creates a tensor with all elements set to zero.\n",
        "        # The condition tensor acts as a mask that chooses, based on the value at each element, \n",
        "        # whether the corresponding element / row in the output should be taken from x (if true) or y (if false).\n",
        "        num_train_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # number of non zero values in the training set\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) # boolean mask\n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs)) # set the output values to zero if corresponding input values are zero\n",
        "        # ? why set output values to zero? does'nt it change the output?\n",
        "\n",
        "        MSE_loss = self._compute_loss(outputs,x,num_train_labels)\n",
        "        \n",
        "        if self.FLAGS['l2_reg'] == True:\n",
        "            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) # Returns all variables created with trainable=True.\n",
        "            MSE_loss = MSE_loss +  self.FLAGS['lambda_'] * l2_loss\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.FLAGS['learning_rate']).minimize(MSE_loss) #An Operation that updates the variables in var_list\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "\n",
        "        return train_op, RMSE_loss\n",
        "    \n",
        "    def _validation_loss(self, x_train, x_test):\n",
        "        ''' Computing the loss during the validation time.\n",
        "            \n",
        "          @param x_train: training data samples\n",
        "          @param x_test: test data samples\n",
        "            \n",
        "          @return networks predictions\n",
        "          @return root mean squared error loss between the predicted and actual ratings\n",
        "          '''\n",
        "        \n",
        "        outputs = self._inference(x_train) # use training sample to make prediction\n",
        "        mask = tf.where(tf.equal(x_test,0.0), tf.zeros_like(x_test), x_test) # identify the zero values in the test ste\n",
        "        num_test_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # count the number of non zero values\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) \n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs))\n",
        "    \n",
        "        MSE_loss = self._compute_loss(outputs, x_test, num_test_labels)\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "            \n",
        "        return outputs, RMSE_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylCWKRtHCAyw",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR286YkxBgT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "    '''Building the graph, opening of a session and starting the training od the neural network.'''\n",
        "    \n",
        "    num_batches = int(FLAGS['num_samples']/FLAGS['batch_size'])\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "\n",
        "        train_data, train_data_infer = _get_training_data(FLAGS)\n",
        "        test_data = _get_test_data(FLAGS)\n",
        "        \n",
        "        iter_train = train_data.make_initializable_iterator()\n",
        "        #Creates a tf.data.Iterator for enumerating the elements of a dataset.\n",
        "        iter_train_infer = train_data_infer.make_initializable_iterator()\n",
        "        iter_test = test_data.make_initializable_iterator()\n",
        "        \n",
        "        x_train = iter_train.get_next() #Returns a nested structure of tf.Tensors representing the next element.\n",
        "        x_train_infer = iter_train_infer.get_next()\n",
        "        x_test = iter_test.get_next()\n",
        "\n",
        "        model = DAE(FLAGS)\n",
        "\n",
        "        train_op, train_loss_op = model._optimizer(x_train)\n",
        "        pred_op, test_loss_op = model._validation_loss(x_train_infer, x_test)\n",
        "       \n",
        "        with tf.Session() as sess: #A class for running TensorFlow operations\n",
        "            \n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            train_loss = 0\n",
        "            test_loss = 0\n",
        "\n",
        "            for epoch in range(FLAGS['num_epoch']):\n",
        "                \n",
        "                sess.run(iter_train.initializer) #The returned iterator will be in an uninitialized state, \n",
        "                                                 #and you must run the iterator.initializer operation before using it\n",
        "                \n",
        "                for batch_nr in range(num_batches):\n",
        "                    \n",
        "                    _, loss_ = sess.run((train_op, train_loss_op))\n",
        "                    train_loss += loss_\n",
        "              \n",
        "                sess.run(iter_train_infer.initializer)\n",
        "                sess.run(iter_test.initializer)\n",
        "\n",
        "                for i in range(FLAGS['num_samples']):\n",
        "                    pred, loss_ = sess.run((pred_op, test_loss_op))\n",
        "                    test_loss += loss_\n",
        "                    \n",
        "                print('epoch_nr: %i, train_loss: %.3f, test_loss: %.3f'%(epoch,(train_loss/num_batches), (test_loss/FLAGS['num_samples'])))\n",
        "                train_loss = 0\n",
        "                test_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5V32k3AFmGj",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Sigmoid Function without  L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFghgRSbFpoq",
        "colab_type": "code",
        "outputId": "53ea37b4-48cd-4004-98e1-ebc60a6737fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l2_reg': False,  # L2 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 128,  # Number of hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train(FLAGS)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 1.339, test_loss: 0.978\n",
            "epoch_nr: 1, train_loss: 0.989, test_loss: 0.975\n",
            "epoch_nr: 2, train_loss: 0.991, test_loss: 0.973\n",
            "epoch_nr: 3, train_loss: 0.990, test_loss: 0.976\n",
            "epoch_nr: 4, train_loss: 0.992, test_loss: 0.972\n",
            "epoch_nr: 5, train_loss: 0.984, test_loss: 0.979\n",
            "epoch_nr: 6, train_loss: 0.968, test_loss: 0.985\n",
            "epoch_nr: 7, train_loss: 0.955, test_loss: 0.994\n",
            "epoch_nr: 8, train_loss: 0.941, test_loss: 1.006\n",
            "epoch_nr: 9, train_loss: 0.931, test_loss: 1.008\n",
            "epoch_nr: 10, train_loss: 0.926, test_loss: 1.012\n",
            "epoch_nr: 11, train_loss: 0.924, test_loss: 1.014\n",
            "epoch_nr: 12, train_loss: 0.920, test_loss: 1.017\n",
            "epoch_nr: 13, train_loss: 0.918, test_loss: 1.020\n",
            "epoch_nr: 14, train_loss: 0.918, test_loss: 1.019\n",
            "epoch_nr: 15, train_loss: 0.915, test_loss: 1.021\n",
            "epoch_nr: 16, train_loss: 0.908, test_loss: 1.029\n",
            "epoch_nr: 17, train_loss: 0.901, test_loss: 1.036\n",
            "epoch_nr: 18, train_loss: 0.895, test_loss: 1.043\n",
            "epoch_nr: 19, train_loss: 0.891, test_loss: 1.047\n",
            "epoch_nr: 20, train_loss: 0.888, test_loss: 1.049\n",
            "epoch_nr: 21, train_loss: 0.884, test_loss: 1.054\n",
            "epoch_nr: 22, train_loss: 0.880, test_loss: 1.055\n",
            "epoch_nr: 23, train_loss: 0.879, test_loss: 1.058\n",
            "epoch_nr: 24, train_loss: 0.876, test_loss: 1.058\n",
            "epoch_nr: 25, train_loss: 0.874, test_loss: 1.062\n",
            "epoch_nr: 26, train_loss: 0.872, test_loss: 1.065\n",
            "epoch_nr: 27, train_loss: 0.868, test_loss: 1.069\n",
            "epoch_nr: 28, train_loss: 0.865, test_loss: 1.074\n",
            "epoch_nr: 29, train_loss: 0.860, test_loss: 1.076\n",
            "epoch_nr: 30, train_loss: 0.858, test_loss: 1.077\n",
            "epoch_nr: 31, train_loss: 0.855, test_loss: 1.083\n",
            "epoch_nr: 32, train_loss: 0.853, test_loss: 1.086\n",
            "epoch_nr: 33, train_loss: 0.850, test_loss: 1.087\n",
            "epoch_nr: 34, train_loss: 0.848, test_loss: 1.088\n",
            "epoch_nr: 35, train_loss: 0.846, test_loss: 1.089\n",
            "epoch_nr: 36, train_loss: 0.843, test_loss: 1.092\n",
            "epoch_nr: 37, train_loss: 0.841, test_loss: 1.095\n",
            "epoch_nr: 38, train_loss: 0.840, test_loss: 1.098\n",
            "epoch_nr: 39, train_loss: 0.838, test_loss: 1.103\n",
            "epoch_nr: 40, train_loss: 0.836, test_loss: 1.100\n",
            "epoch_nr: 41, train_loss: 0.834, test_loss: 1.107\n",
            "epoch_nr: 42, train_loss: 0.832, test_loss: 1.102\n",
            "epoch_nr: 43, train_loss: 0.830, test_loss: 1.105\n",
            "epoch_nr: 44, train_loss: 0.829, test_loss: 1.111\n",
            "epoch_nr: 45, train_loss: 0.826, test_loss: 1.109\n",
            "epoch_nr: 46, train_loss: 0.825, test_loss: 1.112\n",
            "epoch_nr: 47, train_loss: 0.823, test_loss: 1.117\n",
            "epoch_nr: 48, train_loss: 0.821, test_loss: 1.120\n",
            "epoch_nr: 49, train_loss: 0.818, test_loss: 1.125\n",
            "epoch_nr: 50, train_loss: 0.816, test_loss: 1.122\n",
            "epoch_nr: 51, train_loss: 0.814, test_loss: 1.126\n",
            "epoch_nr: 52, train_loss: 0.812, test_loss: 1.131\n",
            "epoch_nr: 53, train_loss: 0.809, test_loss: 1.131\n",
            "epoch_nr: 54, train_loss: 0.807, test_loss: 1.136\n",
            "epoch_nr: 55, train_loss: 0.805, test_loss: 1.135\n",
            "epoch_nr: 56, train_loss: 0.802, test_loss: 1.141\n",
            "epoch_nr: 57, train_loss: 0.801, test_loss: 1.140\n",
            "epoch_nr: 58, train_loss: 0.799, test_loss: 1.143\n",
            "epoch_nr: 59, train_loss: 0.796, test_loss: 1.143\n",
            "epoch_nr: 60, train_loss: 0.794, test_loss: 1.146\n",
            "epoch_nr: 61, train_loss: 0.794, test_loss: 1.152\n",
            "epoch_nr: 62, train_loss: 0.792, test_loss: 1.155\n",
            "epoch_nr: 63, train_loss: 0.788, test_loss: 1.154\n",
            "epoch_nr: 64, train_loss: 0.786, test_loss: 1.153\n",
            "epoch_nr: 65, train_loss: 0.785, test_loss: 1.160\n",
            "epoch_nr: 66, train_loss: 0.781, test_loss: 1.159\n",
            "epoch_nr: 67, train_loss: 0.780, test_loss: 1.159\n",
            "epoch_nr: 68, train_loss: 0.778, test_loss: 1.160\n",
            "epoch_nr: 69, train_loss: 0.776, test_loss: 1.166\n",
            "epoch_nr: 70, train_loss: 0.774, test_loss: 1.163\n",
            "epoch_nr: 71, train_loss: 0.771, test_loss: 1.169\n",
            "epoch_nr: 72, train_loss: 0.770, test_loss: 1.171\n",
            "epoch_nr: 73, train_loss: 0.768, test_loss: 1.172\n",
            "epoch_nr: 74, train_loss: 0.765, test_loss: 1.169\n",
            "epoch_nr: 75, train_loss: 0.763, test_loss: 1.176\n",
            "epoch_nr: 76, train_loss: 0.763, test_loss: 1.175\n",
            "epoch_nr: 77, train_loss: 0.761, test_loss: 1.177\n",
            "epoch_nr: 78, train_loss: 0.758, test_loss: 1.179\n",
            "epoch_nr: 79, train_loss: 0.755, test_loss: 1.180\n",
            "epoch_nr: 80, train_loss: 0.754, test_loss: 1.181\n",
            "epoch_nr: 81, train_loss: 0.752, test_loss: 1.184\n",
            "epoch_nr: 82, train_loss: 0.749, test_loss: 1.185\n",
            "epoch_nr: 83, train_loss: 0.748, test_loss: 1.183\n",
            "epoch_nr: 84, train_loss: 0.744, test_loss: 1.188\n",
            "epoch_nr: 85, train_loss: 0.742, test_loss: 1.185\n",
            "epoch_nr: 86, train_loss: 0.739, test_loss: 1.190\n",
            "epoch_nr: 87, train_loss: 0.737, test_loss: 1.190\n",
            "epoch_nr: 88, train_loss: 0.735, test_loss: 1.193\n",
            "epoch_nr: 89, train_loss: 0.733, test_loss: 1.198\n",
            "epoch_nr: 90, train_loss: 0.733, test_loss: 1.200\n",
            "epoch_nr: 91, train_loss: 0.729, test_loss: 1.199\n",
            "epoch_nr: 92, train_loss: 0.728, test_loss: 1.199\n",
            "epoch_nr: 93, train_loss: 0.725, test_loss: 1.202\n",
            "epoch_nr: 94, train_loss: 0.723, test_loss: 1.203\n",
            "epoch_nr: 95, train_loss: 0.722, test_loss: 1.202\n",
            "epoch_nr: 96, train_loss: 0.719, test_loss: 1.200\n",
            "epoch_nr: 97, train_loss: 0.716, test_loss: 1.206\n",
            "epoch_nr: 98, train_loss: 0.715, test_loss: 1.209\n",
            "epoch_nr: 99, train_loss: 0.713, test_loss: 1.209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKA1FL95FgZl",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Sigmoid Function with L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCqzPvw5h0QN",
        "colab_type": "code",
        "outputId": "c2fac565-707d-498c-9884-911c9f0138da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l2_reg': True,  # L2 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 128,  # Number of hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train(FLAGS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 2.441, test_loss: 1.092\n",
            "epoch_nr: 1, train_loss: 1.563, test_loss: 1.088\n",
            "epoch_nr: 2, train_loss: 1.504, test_loss: 1.088\n",
            "epoch_nr: 3, train_loss: 1.498, test_loss: 1.094\n",
            "epoch_nr: 4, train_loss: 1.497, test_loss: 1.090\n",
            "epoch_nr: 5, train_loss: 1.496, test_loss: 1.089\n",
            "epoch_nr: 6, train_loss: 1.496, test_loss: 1.083\n",
            "epoch_nr: 7, train_loss: 1.498, test_loss: 1.093\n",
            "epoch_nr: 8, train_loss: 1.497, test_loss: 1.088\n",
            "epoch_nr: 9, train_loss: 1.497, test_loss: 1.097\n",
            "epoch_nr: 10, train_loss: 1.498, test_loss: 1.086\n",
            "epoch_nr: 11, train_loss: 1.497, test_loss: 1.091\n",
            "epoch_nr: 12, train_loss: 1.500, test_loss: 1.083\n",
            "epoch_nr: 13, train_loss: 1.496, test_loss: 1.087\n",
            "epoch_nr: 14, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 15, train_loss: 1.496, test_loss: 1.095\n",
            "epoch_nr: 16, train_loss: 1.497, test_loss: 1.080\n",
            "epoch_nr: 17, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 18, train_loss: 1.497, test_loss: 1.083\n",
            "epoch_nr: 19, train_loss: 1.498, test_loss: 1.083\n",
            "epoch_nr: 20, train_loss: 1.498, test_loss: 1.090\n",
            "epoch_nr: 21, train_loss: 1.497, test_loss: 1.091\n",
            "epoch_nr: 22, train_loss: 1.498, test_loss: 1.088\n",
            "epoch_nr: 23, train_loss: 1.497, test_loss: 1.089\n",
            "epoch_nr: 24, train_loss: 1.498, test_loss: 1.083\n",
            "epoch_nr: 25, train_loss: 1.497, test_loss: 1.088\n",
            "epoch_nr: 26, train_loss: 1.498, test_loss: 1.089\n",
            "epoch_nr: 27, train_loss: 1.497, test_loss: 1.087\n",
            "epoch_nr: 28, train_loss: 1.498, test_loss: 1.096\n",
            "epoch_nr: 29, train_loss: 1.497, test_loss: 1.092\n",
            "epoch_nr: 30, train_loss: 1.498, test_loss: 1.086\n",
            "epoch_nr: 31, train_loss: 1.497, test_loss: 1.089\n",
            "epoch_nr: 32, train_loss: 1.498, test_loss: 1.081\n",
            "epoch_nr: 33, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 34, train_loss: 1.498, test_loss: 1.097\n",
            "epoch_nr: 35, train_loss: 1.497, test_loss: 1.091\n",
            "epoch_nr: 36, train_loss: 1.497, test_loss: 1.088\n",
            "epoch_nr: 37, train_loss: 1.497, test_loss: 1.093\n",
            "epoch_nr: 38, train_loss: 1.497, test_loss: 1.089\n",
            "epoch_nr: 39, train_loss: 1.498, test_loss: 1.089\n",
            "epoch_nr: 40, train_loss: 1.497, test_loss: 1.085\n",
            "epoch_nr: 41, train_loss: 1.497, test_loss: 1.098\n",
            "epoch_nr: 42, train_loss: 1.498, test_loss: 1.083\n",
            "epoch_nr: 43, train_loss: 1.497, test_loss: 1.092\n",
            "epoch_nr: 44, train_loss: 1.498, test_loss: 1.091\n",
            "epoch_nr: 45, train_loss: 1.498, test_loss: 1.089\n",
            "epoch_nr: 46, train_loss: 1.497, test_loss: 1.090\n",
            "epoch_nr: 47, train_loss: 1.498, test_loss: 1.094\n",
            "epoch_nr: 48, train_loss: 1.496, test_loss: 1.091\n",
            "epoch_nr: 49, train_loss: 1.497, test_loss: 1.091\n",
            "epoch_nr: 50, train_loss: 1.497, test_loss: 1.094\n",
            "epoch_nr: 51, train_loss: 1.497, test_loss: 1.080\n",
            "epoch_nr: 52, train_loss: 1.496, test_loss: 1.084\n",
            "epoch_nr: 53, train_loss: 1.496, test_loss: 1.095\n",
            "epoch_nr: 54, train_loss: 1.497, test_loss: 1.092\n",
            "epoch_nr: 55, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 56, train_loss: 1.497, test_loss: 1.092\n",
            "epoch_nr: 57, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 58, train_loss: 1.497, test_loss: 1.082\n",
            "epoch_nr: 59, train_loss: 1.497, test_loss: 1.096\n",
            "epoch_nr: 60, train_loss: 1.496, test_loss: 1.095\n",
            "epoch_nr: 61, train_loss: 1.498, test_loss: 1.089\n",
            "epoch_nr: 62, train_loss: 1.497, test_loss: 1.096\n",
            "epoch_nr: 63, train_loss: 1.496, test_loss: 1.092\n",
            "epoch_nr: 64, train_loss: 1.498, test_loss: 1.077\n",
            "epoch_nr: 65, train_loss: 1.497, test_loss: 1.090\n",
            "epoch_nr: 66, train_loss: 1.497, test_loss: 1.095\n",
            "epoch_nr: 67, train_loss: 1.497, test_loss: 1.083\n",
            "epoch_nr: 68, train_loss: 1.496, test_loss: 1.083\n",
            "epoch_nr: 69, train_loss: 1.499, test_loss: 1.091\n",
            "epoch_nr: 70, train_loss: 1.498, test_loss: 1.084\n",
            "epoch_nr: 71, train_loss: 1.499, test_loss: 1.085\n",
            "epoch_nr: 72, train_loss: 1.497, test_loss: 1.081\n",
            "epoch_nr: 73, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 74, train_loss: 1.496, test_loss: 1.083\n",
            "epoch_nr: 75, train_loss: 1.496, test_loss: 1.096\n",
            "epoch_nr: 76, train_loss: 1.498, test_loss: 1.098\n",
            "epoch_nr: 77, train_loss: 1.498, test_loss: 1.087\n",
            "epoch_nr: 78, train_loss: 1.499, test_loss: 1.078\n",
            "epoch_nr: 79, train_loss: 1.498, test_loss: 1.090\n",
            "epoch_nr: 80, train_loss: 1.498, test_loss: 1.083\n",
            "epoch_nr: 81, train_loss: 1.496, test_loss: 1.090\n",
            "epoch_nr: 82, train_loss: 1.497, test_loss: 1.084\n",
            "epoch_nr: 83, train_loss: 1.497, test_loss: 1.091\n",
            "epoch_nr: 84, train_loss: 1.498, test_loss: 1.096\n",
            "epoch_nr: 85, train_loss: 1.498, test_loss: 1.095\n",
            "epoch_nr: 86, train_loss: 1.496, test_loss: 1.085\n",
            "epoch_nr: 87, train_loss: 1.498, test_loss: 1.103\n",
            "epoch_nr: 88, train_loss: 1.496, test_loss: 1.088\n",
            "epoch_nr: 89, train_loss: 1.497, test_loss: 1.084\n",
            "epoch_nr: 90, train_loss: 1.500, test_loss: 1.089\n",
            "epoch_nr: 91, train_loss: 1.499, test_loss: 1.079\n",
            "epoch_nr: 92, train_loss: 1.497, test_loss: 1.099\n",
            "epoch_nr: 93, train_loss: 1.497, test_loss: 1.096\n",
            "epoch_nr: 94, train_loss: 1.496, test_loss: 1.093\n",
            "epoch_nr: 95, train_loss: 1.497, test_loss: 1.091\n",
            "epoch_nr: 96, train_loss: 1.497, test_loss: 1.086\n",
            "epoch_nr: 97, train_loss: 1.498, test_loss: 1.094\n",
            "epoch_nr: 98, train_loss: 1.497, test_loss: 1.088\n",
            "epoch_nr: 99, train_loss: 1.498, test_loss: 1.090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg99Do2NFQ3Q",
        "colab_type": "text"
      },
      "source": [
        "### 2. ReLU Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxdEr_y3v-4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import model_helper\n",
        "\n",
        "\n",
        "def _get_bias_initializer():\n",
        "    return tf.zeros_initializer()\n",
        "\n",
        "\n",
        "def _get_weight_initializer():\n",
        "    return tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
        "\n",
        "\n",
        "class DAE_ReLU: #Data Acquisition Engine\n",
        "    \n",
        "    def __init__(self, FLAGS):\n",
        "        ''' Implementation of deep autoencoder class.'''\n",
        "        \n",
        "        self.FLAGS = FLAGS\n",
        "        self.weight_initializer = _get_weight_initializer()\n",
        "        self.bias_initializer = _get_bias_initializer()\n",
        "        self.init_parameters()\n",
        "        \n",
        "\n",
        "    def init_parameters(self):\n",
        "        '''Initialize networks weights and biasis.'''\n",
        "        \n",
        "        with tf.name_scope('weights'):\n",
        "          #This context manager validates that the given values are from the same graph,  \n",
        "          #makes that graph the default graph, and pushes a name scope in that graph\n",
        "            self.W_1 = tf.get_variable(name='weight_1', shape=(self.FLAGS['num_v'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer) #Gets an existing variable with these parameters or create a new one\n",
        "            self.W_2 = tf.get_variable(name='weight_2', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_3 = tf.get_variable(name='weight_3', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_4 = tf.get_variable(name='weight_4', shape=(self.FLAGS['num_h'], self.FLAGS['num_v']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "        \n",
        "        with tf.name_scope('biases'):\n",
        "            self.b1 = tf.get_variable(name='bias_1', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b2 = tf.get_variable(name='bias_2', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b3 = tf.get_variable(name='bias_3', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "        \n",
        "    def _inference(self, x):\n",
        "        ''' Making one forward pass. Predicting the networks outputs.\n",
        "        @param x: input ratings\n",
        "        \n",
        "        @return : networks predictions\n",
        "        '''\\\n",
        "        \n",
        "        with tf.name_scope('inference'):\n",
        "             a1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, self.W_1),self.b1)) # sign(W1T*X+b1)\n",
        "             a2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a1, self.W_2),self.b2))\n",
        "             a3 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a2, self.W_3),self.b3))\n",
        "             a4 = tf.matmul(a3, self.W_4)\n",
        "        return a4\n",
        "    \n",
        "    def _compute_loss(self, predictions, labels, num_labels):\n",
        "        ''' Computing the Mean Squared Error loss between the input and output of the network.\n",
        "            \n",
        "          @param predictions: predictions of the stacked autoencoder\n",
        "          @param labels: input values of the stacked autoencoder which serve as labels at the same time\n",
        "          @param num_labels: number of labels !=0 in the data set to compute the mean\n",
        "            \n",
        "          @return mean squared error loss tf-operation\n",
        "          '''\n",
        "            \n",
        "        with tf.name_scope('loss'):\n",
        "            loss_op = tf.div(tf.reduce_sum(tf.square(tf.subtract(predictions,labels))),num_labels)\n",
        "            return loss_op\n",
        "          \n",
        "        \n",
        "\n",
        "    def _optimizer(self, x):\n",
        "        '''Optimization of the network parameter through stochastic gradient descent.\n",
        "            \n",
        "            @param x: input values for the stacked autoencoder.\n",
        "            \n",
        "            @return: tensorflow training operation\n",
        "            @return: ROOT!! mean squared error\n",
        "        '''\n",
        "        \n",
        "        outputs = self._inference(x)\n",
        "        # ? the mask is same as x ?\n",
        "        mask = tf.where(tf.equal(x,0.0), tf.zeros_like(x), x) # indices of 0 values in the training set\n",
        "        # tf.zero_like : Creates a tensor with all elements set to zero.\n",
        "        # The condition tensor acts as a mask that chooses, based on the value at each element, \n",
        "        # whether the corresponding element / row in the output should be taken from x (if true) or y (if false).\n",
        "        num_train_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # number of non zero values in the training set\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) # boolean mask\n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs)) # set the output values to zero if corresponding input values are zero\n",
        "        # ? why set output values to zero? does'nt it change the output?\n",
        "\n",
        "        MSE_loss = self._compute_loss(outputs,x,num_train_labels)\n",
        "        \n",
        "        if self.FLAGS['l2_reg'] == True:\n",
        "            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) # Returns all variables created with trainable=True.\n",
        "            MSE_loss = MSE_loss +  self.FLAGS['lambda_'] * l2_loss\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.FLAGS['learning_rate']).minimize(MSE_loss) #An Operation that updates the variables in var_list\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "\n",
        "        return train_op, RMSE_loss\n",
        "    \n",
        "    def _validation_loss(self, x_train, x_test):\n",
        "        ''' Computing the loss during the validation time.\n",
        "            \n",
        "          @param x_train: training data samples\n",
        "          @param x_test: test data samples\n",
        "            \n",
        "          @return networks predictions\n",
        "          @return root mean squared error loss between the predicted and actual ratings\n",
        "          '''\n",
        "        \n",
        "        outputs = self._inference(x_train) # use training sample to make prediction\n",
        "        mask = tf.where(tf.equal(x_test,0.0), tf.zeros_like(x_test), x_test) # identify the zero values in the test ste\n",
        "        num_test_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # count the number of non zero values\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) \n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs))\n",
        "    \n",
        "        MSE_loss = self._compute_loss(outputs, x_test, num_test_labels)\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "            \n",
        "        return outputs, RMSE_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvj5TFV6GhZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def train_with_ReLU(FLAGS):\n",
        "    '''Building the graph, opening of a session and starting the training od the neural network.'''\n",
        "    \n",
        "    num_batches = int(FLAGS['num_samples']/FLAGS['batch_size'])\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "\n",
        "        train_data, train_data_infer = _get_training_data(FLAGS)\n",
        "        test_data = _get_test_data(FLAGS)\n",
        "        \n",
        "        iter_train = train_data.make_initializable_iterator()\n",
        "        #Creates a tf.data.Iterator for enumerating the elements of a dataset.\n",
        "        iter_train_infer = train_data_infer.make_initializable_iterator()\n",
        "        iter_test = test_data.make_initializable_iterator()\n",
        "        \n",
        "        x_train = iter_train.get_next() #Returns a nested structure of tf.Tensors representing the next element.\n",
        "        x_train_infer = iter_train_infer.get_next()\n",
        "        x_test = iter_test.get_next()\n",
        "\n",
        "        model = DAE_ReLU(FLAGS)\n",
        "\n",
        "        train_op, train_loss_op = model._optimizer(x_train)\n",
        "        pred_op, test_loss_op = model._validation_loss(x_train_infer, x_test)\n",
        "       \n",
        "        with tf.Session() as sess: #A class for running TensorFlow operations\n",
        "            \n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            train_loss = 0\n",
        "            test_loss = 0\n",
        "\n",
        "            for epoch in range(FLAGS['num_epoch']):\n",
        "                \n",
        "                sess.run(iter_train.initializer) #The returned iterator will be in an uninitialized state, \n",
        "                                                 #and you must run the iterator.initializer operation before using it\n",
        "                \n",
        "                for batch_nr in range(num_batches):\n",
        "                    \n",
        "                    _, loss_ = sess.run((train_op, train_loss_op))\n",
        "                    train_loss += loss_\n",
        "              \n",
        "                sess.run(iter_train_infer.initializer)\n",
        "                sess.run(iter_test.initializer)\n",
        "\n",
        "                for i in range(FLAGS['num_samples']):\n",
        "                    pred, loss_ = sess.run((pred_op, test_loss_op))\n",
        "                    test_loss += loss_\n",
        "                    \n",
        "                print('epoch_nr: %i, train_loss: %.3f, test_loss: %.3f'%(epoch,(train_loss/num_batches), (test_loss/FLAGS['num_samples'])))\n",
        "                train_loss = 0\n",
        "                test_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_yIXjsiGVKS",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. ReLU Activation Function without  L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2DnBLcJGOqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c83f405-7c2c-4560-8117-b940b805e04f"
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l2_reg': False,  # L2 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 128,  # Number of hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train_with_ReLU(FLAGS)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 1.493, test_loss: 1.180\n",
            "epoch_nr: 1, train_loss: 1.085, test_loss: 1.109\n",
            "epoch_nr: 2, train_loss: 1.018, test_loss: 1.079\n",
            "epoch_nr: 3, train_loss: 0.987, test_loss: 1.076\n",
            "epoch_nr: 4, train_loss: 0.952, test_loss: 1.065\n",
            "epoch_nr: 5, train_loss: 0.930, test_loss: 1.069\n",
            "epoch_nr: 6, train_loss: 0.920, test_loss: 1.079\n",
            "epoch_nr: 7, train_loss: 0.910, test_loss: 1.078\n",
            "epoch_nr: 8, train_loss: 0.903, test_loss: 1.082\n",
            "epoch_nr: 9, train_loss: 0.893, test_loss: 1.082\n",
            "epoch_nr: 10, train_loss: 0.883, test_loss: 1.093\n",
            "epoch_nr: 11, train_loss: 0.870, test_loss: 1.091\n",
            "epoch_nr: 12, train_loss: 0.862, test_loss: 1.097\n",
            "epoch_nr: 13, train_loss: 0.854, test_loss: 1.105\n",
            "epoch_nr: 14, train_loss: 0.854, test_loss: 1.104\n",
            "epoch_nr: 15, train_loss: 0.846, test_loss: 1.123\n",
            "epoch_nr: 16, train_loss: 0.836, test_loss: 1.131\n",
            "epoch_nr: 17, train_loss: 0.826, test_loss: 1.124\n",
            "epoch_nr: 18, train_loss: 0.810, test_loss: 1.116\n",
            "epoch_nr: 19, train_loss: 0.797, test_loss: 1.133\n",
            "epoch_nr: 20, train_loss: 0.786, test_loss: 1.125\n",
            "epoch_nr: 21, train_loss: 0.777, test_loss: 1.143\n",
            "epoch_nr: 22, train_loss: 0.771, test_loss: 1.149\n",
            "epoch_nr: 23, train_loss: 0.766, test_loss: 1.144\n",
            "epoch_nr: 24, train_loss: 0.762, test_loss: 1.175\n",
            "epoch_nr: 25, train_loss: 0.750, test_loss: 1.192\n",
            "epoch_nr: 26, train_loss: 0.738, test_loss: 1.177\n",
            "epoch_nr: 27, train_loss: 0.732, test_loss: 1.191\n",
            "epoch_nr: 28, train_loss: 0.723, test_loss: 1.206\n",
            "epoch_nr: 29, train_loss: 0.715, test_loss: 1.196\n",
            "epoch_nr: 30, train_loss: 0.711, test_loss: 1.192\n",
            "epoch_nr: 31, train_loss: 0.702, test_loss: 1.213\n",
            "epoch_nr: 32, train_loss: 0.694, test_loss: 1.217\n",
            "epoch_nr: 33, train_loss: 0.685, test_loss: 1.243\n",
            "epoch_nr: 34, train_loss: 0.681, test_loss: 1.233\n",
            "epoch_nr: 35, train_loss: 0.675, test_loss: 1.219\n",
            "epoch_nr: 36, train_loss: 0.669, test_loss: 1.226\n",
            "epoch_nr: 37, train_loss: 0.662, test_loss: 1.232\n",
            "epoch_nr: 38, train_loss: 0.656, test_loss: 1.252\n",
            "epoch_nr: 39, train_loss: 0.649, test_loss: 1.251\n",
            "epoch_nr: 40, train_loss: 0.645, test_loss: 1.251\n",
            "epoch_nr: 41, train_loss: 0.643, test_loss: 1.255\n",
            "epoch_nr: 42, train_loss: 0.638, test_loss: 1.267\n",
            "epoch_nr: 43, train_loss: 0.630, test_loss: 1.268\n",
            "epoch_nr: 44, train_loss: 0.624, test_loss: 1.262\n",
            "epoch_nr: 45, train_loss: 0.624, test_loss: 1.293\n",
            "epoch_nr: 46, train_loss: 0.622, test_loss: 1.272\n",
            "epoch_nr: 47, train_loss: 0.617, test_loss: 1.288\n",
            "epoch_nr: 48, train_loss: 0.608, test_loss: 1.277\n",
            "epoch_nr: 49, train_loss: 0.603, test_loss: 1.298\n",
            "epoch_nr: 50, train_loss: 0.601, test_loss: 1.295\n",
            "epoch_nr: 51, train_loss: 0.596, test_loss: 1.296\n",
            "epoch_nr: 52, train_loss: 0.594, test_loss: 1.294\n",
            "epoch_nr: 53, train_loss: 0.592, test_loss: 1.298\n",
            "epoch_nr: 54, train_loss: 0.591, test_loss: 1.316\n",
            "epoch_nr: 55, train_loss: 0.588, test_loss: 1.298\n",
            "epoch_nr: 56, train_loss: 0.587, test_loss: 1.334\n",
            "epoch_nr: 57, train_loss: 0.580, test_loss: 1.325\n",
            "epoch_nr: 58, train_loss: 0.577, test_loss: 1.321\n",
            "epoch_nr: 59, train_loss: 0.573, test_loss: 1.331\n",
            "epoch_nr: 60, train_loss: 0.569, test_loss: 1.325\n",
            "epoch_nr: 61, train_loss: 0.572, test_loss: 1.319\n",
            "epoch_nr: 62, train_loss: 0.571, test_loss: 1.349\n",
            "epoch_nr: 63, train_loss: 0.565, test_loss: 1.322\n",
            "epoch_nr: 64, train_loss: 0.561, test_loss: 1.326\n",
            "epoch_nr: 65, train_loss: 0.557, test_loss: 1.344\n",
            "epoch_nr: 66, train_loss: 0.555, test_loss: 1.334\n",
            "epoch_nr: 67, train_loss: 0.554, test_loss: 1.349\n",
            "epoch_nr: 68, train_loss: 0.554, test_loss: 1.326\n",
            "epoch_nr: 69, train_loss: 0.554, test_loss: 1.332\n",
            "epoch_nr: 70, train_loss: 0.553, test_loss: 1.354\n",
            "epoch_nr: 71, train_loss: 0.549, test_loss: 1.345\n",
            "epoch_nr: 72, train_loss: 0.546, test_loss: 1.368\n",
            "epoch_nr: 73, train_loss: 0.541, test_loss: 1.368\n",
            "epoch_nr: 74, train_loss: 0.537, test_loss: 1.355\n",
            "epoch_nr: 75, train_loss: 0.536, test_loss: 1.354\n",
            "epoch_nr: 76, train_loss: 0.537, test_loss: 1.347\n",
            "epoch_nr: 77, train_loss: 0.540, test_loss: 1.352\n",
            "epoch_nr: 78, train_loss: 0.538, test_loss: 1.363\n",
            "epoch_nr: 79, train_loss: 0.537, test_loss: 1.362\n",
            "epoch_nr: 80, train_loss: 0.533, test_loss: 1.367\n",
            "epoch_nr: 81, train_loss: 0.528, test_loss: 1.363\n",
            "epoch_nr: 82, train_loss: 0.528, test_loss: 1.365\n",
            "epoch_nr: 83, train_loss: 0.529, test_loss: 1.372\n",
            "epoch_nr: 84, train_loss: 0.534, test_loss: 1.363\n",
            "epoch_nr: 85, train_loss: 0.528, test_loss: 1.362\n",
            "epoch_nr: 86, train_loss: 0.523, test_loss: 1.387\n",
            "epoch_nr: 87, train_loss: 0.524, test_loss: 1.364\n",
            "epoch_nr: 88, train_loss: 0.520, test_loss: 1.377\n",
            "epoch_nr: 89, train_loss: 0.520, test_loss: 1.390\n",
            "epoch_nr: 90, train_loss: 0.520, test_loss: 1.376\n",
            "epoch_nr: 91, train_loss: 0.516, test_loss: 1.364\n",
            "epoch_nr: 92, train_loss: 0.513, test_loss: 1.379\n",
            "epoch_nr: 93, train_loss: 0.514, test_loss: 1.379\n",
            "epoch_nr: 94, train_loss: 0.512, test_loss: 1.397\n",
            "epoch_nr: 95, train_loss: 0.512, test_loss: 1.377\n",
            "epoch_nr: 96, train_loss: 0.512, test_loss: 1.392\n",
            "epoch_nr: 97, train_loss: 0.513, test_loss: 1.399\n",
            "epoch_nr: 98, train_loss: 0.508, test_loss: 1.389\n",
            "epoch_nr: 99, train_loss: 0.510, test_loss: 1.386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksuVIxIVHSuJ",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. ReLU Activation Function with  L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJJ-Y4pyGbEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e23d2a1c-e1dd-45e5-923d-9ddea4cf5ef3"
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l2_reg': True,  # L2 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 128,  # Number of hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train_with_ReLU(FLAGS)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 2.808, test_loss: 1.247\n",
            "epoch_nr: 1, train_loss: 2.013, test_loss: 1.177\n",
            "epoch_nr: 2, train_loss: 1.754, test_loss: 1.139\n",
            "epoch_nr: 3, train_loss: 1.592, test_loss: 1.140\n",
            "epoch_nr: 4, train_loss: 1.496, test_loss: 1.165\n",
            "epoch_nr: 5, train_loss: 1.442, test_loss: 1.126\n",
            "epoch_nr: 6, train_loss: 1.395, test_loss: 1.114\n",
            "epoch_nr: 7, train_loss: 1.342, test_loss: 1.127\n",
            "epoch_nr: 8, train_loss: 1.342, test_loss: 1.112\n",
            "epoch_nr: 9, train_loss: 1.308, test_loss: 1.119\n",
            "epoch_nr: 10, train_loss: 1.293, test_loss: 1.113\n",
            "epoch_nr: 11, train_loss: 1.347, test_loss: 1.136\n",
            "epoch_nr: 12, train_loss: 1.306, test_loss: 1.108\n",
            "epoch_nr: 13, train_loss: 1.266, test_loss: 1.112\n",
            "epoch_nr: 14, train_loss: 1.241, test_loss: 1.103\n",
            "epoch_nr: 15, train_loss: 1.237, test_loss: 1.100\n",
            "epoch_nr: 16, train_loss: 1.237, test_loss: 1.111\n",
            "epoch_nr: 17, train_loss: 1.342, test_loss: 1.132\n",
            "epoch_nr: 18, train_loss: 1.260, test_loss: 1.102\n",
            "epoch_nr: 19, train_loss: 1.220, test_loss: 1.095\n",
            "epoch_nr: 20, train_loss: 1.199, test_loss: 1.086\n",
            "epoch_nr: 21, train_loss: 1.201, test_loss: 1.110\n",
            "epoch_nr: 22, train_loss: 1.209, test_loss: 1.106\n",
            "epoch_nr: 23, train_loss: 1.201, test_loss: 1.098\n",
            "epoch_nr: 24, train_loss: 1.188, test_loss: 1.097\n",
            "epoch_nr: 25, train_loss: 1.200, test_loss: 1.111\n",
            "epoch_nr: 26, train_loss: 1.207, test_loss: 1.095\n",
            "epoch_nr: 27, train_loss: 1.190, test_loss: 1.089\n",
            "epoch_nr: 28, train_loss: 1.174, test_loss: 1.092\n",
            "epoch_nr: 29, train_loss: 1.222, test_loss: 1.101\n",
            "epoch_nr: 30, train_loss: 1.196, test_loss: 1.084\n",
            "epoch_nr: 31, train_loss: 1.172, test_loss: 1.081\n",
            "epoch_nr: 32, train_loss: 1.170, test_loss: 1.082\n",
            "epoch_nr: 33, train_loss: 1.164, test_loss: 1.084\n",
            "epoch_nr: 34, train_loss: 1.206, test_loss: 1.104\n",
            "epoch_nr: 35, train_loss: 1.191, test_loss: 1.083\n",
            "epoch_nr: 36, train_loss: 1.165, test_loss: 1.088\n",
            "epoch_nr: 37, train_loss: 1.144, test_loss: 1.073\n",
            "epoch_nr: 38, train_loss: 1.135, test_loss: 1.070\n",
            "epoch_nr: 39, train_loss: 1.141, test_loss: 1.087\n",
            "epoch_nr: 40, train_loss: 1.152, test_loss: 1.080\n",
            "epoch_nr: 41, train_loss: 1.160, test_loss: 1.075\n",
            "epoch_nr: 42, train_loss: 1.161, test_loss: 1.093\n",
            "epoch_nr: 43, train_loss: 1.167, test_loss: 1.079\n",
            "epoch_nr: 44, train_loss: 1.146, test_loss: 1.074\n",
            "epoch_nr: 45, train_loss: 1.135, test_loss: 1.083\n",
            "epoch_nr: 46, train_loss: 1.142, test_loss: 1.081\n",
            "epoch_nr: 47, train_loss: 1.148, test_loss: 1.080\n",
            "epoch_nr: 48, train_loss: 1.144, test_loss: 1.076\n",
            "epoch_nr: 49, train_loss: 1.135, test_loss: 1.077\n",
            "epoch_nr: 50, train_loss: 1.149, test_loss: 1.089\n",
            "epoch_nr: 51, train_loss: 1.141, test_loss: 1.081\n",
            "epoch_nr: 52, train_loss: 1.136, test_loss: 1.076\n",
            "epoch_nr: 53, train_loss: 1.132, test_loss: 1.080\n",
            "epoch_nr: 54, train_loss: 1.129, test_loss: 1.069\n",
            "epoch_nr: 55, train_loss: 1.130, test_loss: 1.079\n",
            "epoch_nr: 56, train_loss: 1.132, test_loss: 1.073\n",
            "epoch_nr: 57, train_loss: 1.126, test_loss: 1.070\n",
            "epoch_nr: 58, train_loss: 1.133, test_loss: 1.078\n",
            "epoch_nr: 59, train_loss: 1.133, test_loss: 1.088\n",
            "epoch_nr: 60, train_loss: 1.126, test_loss: 1.069\n",
            "epoch_nr: 61, train_loss: 1.117, test_loss: 1.071\n",
            "epoch_nr: 62, train_loss: 1.112, test_loss: 1.071\n",
            "epoch_nr: 63, train_loss: 1.120, test_loss: 1.080\n",
            "epoch_nr: 64, train_loss: 1.131, test_loss: 1.076\n",
            "epoch_nr: 65, train_loss: 1.131, test_loss: 1.085\n",
            "epoch_nr: 66, train_loss: 1.121, test_loss: 1.072\n",
            "epoch_nr: 67, train_loss: 1.121, test_loss: 1.069\n",
            "epoch_nr: 68, train_loss: 1.121, test_loss: 1.075\n",
            "epoch_nr: 69, train_loss: 1.123, test_loss: 1.068\n",
            "epoch_nr: 70, train_loss: 1.119, test_loss: 1.066\n",
            "epoch_nr: 71, train_loss: 1.117, test_loss: 1.084\n",
            "epoch_nr: 72, train_loss: 1.120, test_loss: 1.074\n",
            "epoch_nr: 73, train_loss: 1.113, test_loss: 1.073\n",
            "epoch_nr: 74, train_loss: 1.117, test_loss: 1.076\n",
            "epoch_nr: 75, train_loss: 1.127, test_loss: 1.074\n",
            "epoch_nr: 76, train_loss: 1.122, test_loss: 1.078\n",
            "epoch_nr: 77, train_loss: 1.120, test_loss: 1.074\n",
            "epoch_nr: 78, train_loss: 1.115, test_loss: 1.067\n",
            "epoch_nr: 79, train_loss: 1.117, test_loss: 1.068\n",
            "epoch_nr: 80, train_loss: 1.116, test_loss: 1.067\n",
            "epoch_nr: 81, train_loss: 1.114, test_loss: 1.066\n",
            "epoch_nr: 82, train_loss: 1.115, test_loss: 1.070\n",
            "epoch_nr: 83, train_loss: 1.134, test_loss: 1.080\n",
            "epoch_nr: 84, train_loss: 1.126, test_loss: 1.063\n",
            "epoch_nr: 85, train_loss: 1.118, test_loss: 1.060\n",
            "epoch_nr: 86, train_loss: 1.114, test_loss: 1.068\n",
            "epoch_nr: 87, train_loss: 1.114, test_loss: 1.067\n",
            "epoch_nr: 88, train_loss: 1.117, test_loss: 1.066\n",
            "epoch_nr: 89, train_loss: 1.115, test_loss: 1.065\n",
            "epoch_nr: 90, train_loss: 1.113, test_loss: 1.078\n",
            "epoch_nr: 91, train_loss: 1.109, test_loss: 1.076\n",
            "epoch_nr: 92, train_loss: 1.109, test_loss: 1.062\n",
            "epoch_nr: 93, train_loss: 1.109, test_loss: 1.068\n",
            "epoch_nr: 94, train_loss: 1.113, test_loss: 1.063\n",
            "epoch_nr: 95, train_loss: 1.111, test_loss: 1.079\n",
            "epoch_nr: 96, train_loss: 1.112, test_loss: 1.065\n",
            "epoch_nr: 97, train_loss: 1.112, test_loss: 1.060\n",
            "epoch_nr: 98, train_loss: 1.114, test_loss: 1.069\n",
            "epoch_nr: 99, train_loss: 1.107, test_loss: 1.067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P83oKp7MK0y",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. ReLU Activation Function with  L1 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtktMGFpUzk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import model_helper\n",
        "\n",
        "\n",
        "def _get_bias_initializer():\n",
        "    return tf.zeros_initializer()\n",
        "\n",
        "\n",
        "def _get_weight_initializer():\n",
        "    return tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
        "\n",
        "\n",
        "class DAE_ReLU_L1: #Data Acquisition Engine\n",
        "    \n",
        "    def __init__(self, FLAGS):\n",
        "        ''' Implementation of deep autoencoder class.'''\n",
        "        \n",
        "        self.FLAGS = FLAGS\n",
        "        self.weight_initializer = _get_weight_initializer()\n",
        "        self.bias_initializer = _get_bias_initializer()\n",
        "        self.init_parameters()\n",
        "        \n",
        "\n",
        "    def init_parameters(self):\n",
        "        '''Initialize networks weights and biasis.'''\n",
        "        \n",
        "        with tf.name_scope('weights'):\n",
        "          #This context manager validates that the given values are from the same graph,  \n",
        "          #makes that graph the default graph, and pushes a name scope in that graph\n",
        "            self.W_1 = tf.get_variable(name='weight_1', shape=(self.FLAGS['num_v'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer) #Gets an existing variable with these parameters or create a new one\n",
        "            self.W_2 = tf.get_variable(name='weight_2', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_3 = tf.get_variable(name='weight_3', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_4 = tf.get_variable(name='weight_4', shape=(self.FLAGS['num_h'], self.FLAGS['num_v']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "        \n",
        "        with tf.name_scope('biases'):\n",
        "            self.b1 = tf.get_variable(name='bias_1', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b2 = tf.get_variable(name='bias_2', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b3 = tf.get_variable(name='bias_3', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "        \n",
        "    def _inference(self, x):\n",
        "        ''' Making one forward pass. Predicting the networks outputs.\n",
        "        @param x: input ratings\n",
        "        \n",
        "        @return : networks predictions\n",
        "        '''\\\n",
        "        \n",
        "        with tf.name_scope('inference'):\n",
        "             a1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, self.W_1),self.b1)) # sign(W1T*X+b1)\n",
        "             a2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a1, self.W_2),self.b2))\n",
        "             a3 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a2, self.W_3),self.b3))\n",
        "             a4 = tf.matmul(a3, self.W_4)\n",
        "        return a4\n",
        "    \n",
        "    def _compute_loss(self, predictions, labels, num_labels):\n",
        "        ''' Computing the Mean Squared Error loss between the input and output of the network.\n",
        "            \n",
        "          @param predictions: predictions of the stacked autoencoder\n",
        "          @param labels: input values of the stacked autoencoder which serve as labels at the same time\n",
        "          @param num_labels: number of labels !=0 in the data set to compute the mean\n",
        "            \n",
        "          @return mean squared error loss tf-operation\n",
        "          '''\n",
        "            \n",
        "        with tf.name_scope('loss'):\n",
        "            loss_op = tf.div(tf.reduce_sum(tf.square(tf.subtract(predictions,labels))),num_labels)\n",
        "            return loss_op\n",
        "          \n",
        "    def _l1_loss(self, params):\n",
        "        '''\n",
        "        Unfortunately, L1 isn’t a tensorflow function, so we have to create it by ourselves and use it instead of L2\n",
        "        '''\n",
        "        return tf.reduce_sum(tf.abs(params))\n",
        "\n",
        "    def _optimizer(self, x):\n",
        "        '''Optimization of the network parameter through stochastic gradient descent.\n",
        "            \n",
        "            @param x: input values for the stacked autoencoder.\n",
        "            \n",
        "            @return: tensorflow training operation\n",
        "            @return: ROOT!! mean squared error\n",
        "        '''\n",
        "        \n",
        "        outputs = self._inference(x)\n",
        "        # ? the mask is same as x ?\n",
        "        mask = tf.where(tf.equal(x,0.0), tf.zeros_like(x), x) # indices of 0 values in the training set\n",
        "        # tf.zero_like : Creates a tensor with all elements set to zero.\n",
        "        # The condition tensor acts as a mask that chooses, based on the value at each element, \n",
        "        # whether the corresponding element / row in the output should be taken from x (if true) or y (if false).\n",
        "        num_train_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # number of non zero values in the training set\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) # boolean mask\n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs)) # set the output values to zero if corresponding input values are zero\n",
        "        # ? why set output values to zero? does'nt it change the output?\n",
        "\n",
        "        MSE_loss = self._compute_loss(outputs,x,num_train_labels)\n",
        "        \n",
        "        if self.FLAGS['l1_reg'] == True:\n",
        "            l1_loss = tf.add_n([self._l1_loss(v) for v in tf.trainable_variables()]) # Returns all variables created with trainable=True.\n",
        "            MSE_loss = MSE_loss +  self.FLAGS['lambda_'] * l1_loss\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.FLAGS['learning_rate']).minimize(MSE_loss) #An Operation that updates the variables in var_list\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "\n",
        "        return train_op, RMSE_loss\n",
        "    \n",
        "    def _validation_loss(self, x_train, x_test):\n",
        "        ''' Computing the loss during the validation time.\n",
        "            \n",
        "          @param x_train: training data samples\n",
        "          @param x_test: test data samples\n",
        "            \n",
        "          @return networks predictions\n",
        "          @return root mean squared error loss between the predicted and actual ratings\n",
        "          '''\n",
        "        \n",
        "        outputs = self._inference(x_train) # use training sample to make prediction\n",
        "        mask = tf.where(tf.equal(x_test,0.0), tf.zeros_like(x_test), x_test) # identify the zero values in the test ste\n",
        "        num_test_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # count the number of non zero values\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) \n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs))\n",
        "    \n",
        "        MSE_loss = self._compute_loss(outputs, x_test, num_test_labels)\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "            \n",
        "        return outputs, RMSE_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAI07yj-VIjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def train_with_ReLU_L1(FLAGS):\n",
        "    '''Building the graph, opening of a session and starting the training od the neural network.'''\n",
        "    \n",
        "    num_batches = int(FLAGS['num_samples']/FLAGS['batch_size'])\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "\n",
        "        train_data, train_data_infer = _get_training_data(FLAGS)\n",
        "        test_data = _get_test_data(FLAGS)\n",
        "        \n",
        "        iter_train = train_data.make_initializable_iterator()\n",
        "        #Creates a tf.data.Iterator for enumerating the elements of a dataset.\n",
        "        iter_train_infer = train_data_infer.make_initializable_iterator()\n",
        "        iter_test = test_data.make_initializable_iterator()\n",
        "        \n",
        "        x_train = iter_train.get_next() #Returns a nested structure of tf.Tensors representing the next element.\n",
        "        x_train_infer = iter_train_infer.get_next()\n",
        "        x_test = iter_test.get_next()\n",
        "\n",
        "        model = DAE_ReLU_L1(FLAGS)\n",
        "\n",
        "        train_op, train_loss_op = model._optimizer(x_train)\n",
        "        pred_op, test_loss_op = model._validation_loss(x_train_infer, x_test)\n",
        "       \n",
        "        with tf.Session() as sess: #A class for running TensorFlow operations\n",
        "            \n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            train_loss = 0\n",
        "            test_loss = 0\n",
        "\n",
        "            for epoch in range(FLAGS['num_epoch']):\n",
        "                \n",
        "                sess.run(iter_train.initializer) #The returned iterator will be in an uninitialized state, \n",
        "                                                 #and you must run the iterator.initializer operation before using it\n",
        "                \n",
        "                for batch_nr in range(num_batches):\n",
        "                    \n",
        "                    _, loss_ = sess.run((train_op, train_loss_op))\n",
        "                    train_loss += loss_\n",
        "              \n",
        "                sess.run(iter_train_infer.initializer)\n",
        "                sess.run(iter_test.initializer)\n",
        "\n",
        "                for i in range(FLAGS['num_samples']):\n",
        "                    pred, loss_ = sess.run((pred_op, test_loss_op))\n",
        "                    test_loss += loss_\n",
        "                    \n",
        "                print('epoch_nr: %i, train_loss: %.3f, test_loss: %.3f'%(epoch,(train_loss/num_batches), (test_loss/FLAGS['num_samples'])))\n",
        "                train_loss = 0\n",
        "                test_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZVqvd_PJ82s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1de9af7-cdd3-44ab-fdf3-284360f625ea"
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l1_reg': True,  # L1 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 128,  # Number of hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train_with_ReLU_L1(FLAGS)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 7.396, test_loss: 2.374\n",
            "epoch_nr: 1, train_loss: 2.715, test_loss: 2.210\n",
            "epoch_nr: 2, train_loss: 2.565, test_loss: 2.096\n",
            "epoch_nr: 3, train_loss: 2.484, test_loss: 2.055\n",
            "epoch_nr: 4, train_loss: 2.434, test_loss: 2.036\n",
            "epoch_nr: 5, train_loss: 2.391, test_loss: 1.892\n",
            "epoch_nr: 6, train_loss: 2.345, test_loss: 1.832\n",
            "epoch_nr: 7, train_loss: 2.314, test_loss: 1.738\n",
            "epoch_nr: 8, train_loss: 2.259, test_loss: 1.619\n",
            "epoch_nr: 9, train_loss: 2.213, test_loss: 1.501\n",
            "epoch_nr: 10, train_loss: 2.182, test_loss: 1.388\n",
            "epoch_nr: 11, train_loss: 2.130, test_loss: 1.271\n",
            "epoch_nr: 12, train_loss: 2.095, test_loss: 1.225\n",
            "epoch_nr: 13, train_loss: 2.045, test_loss: 1.214\n",
            "epoch_nr: 14, train_loss: 2.051, test_loss: 1.190\n",
            "epoch_nr: 15, train_loss: 1.984, test_loss: 1.111\n",
            "epoch_nr: 16, train_loss: 1.966, test_loss: 1.097\n",
            "epoch_nr: 17, train_loss: 1.975, test_loss: 1.101\n",
            "epoch_nr: 18, train_loss: 1.925, test_loss: 1.107\n",
            "epoch_nr: 19, train_loss: 1.920, test_loss: 1.091\n",
            "epoch_nr: 20, train_loss: 1.934, test_loss: 1.080\n",
            "epoch_nr: 21, train_loss: 1.910, test_loss: 1.104\n",
            "epoch_nr: 22, train_loss: 1.887, test_loss: 1.073\n",
            "epoch_nr: 23, train_loss: 1.900, test_loss: 1.075\n",
            "epoch_nr: 24, train_loss: 1.887, test_loss: 1.065\n",
            "epoch_nr: 25, train_loss: 1.866, test_loss: 1.081\n",
            "epoch_nr: 26, train_loss: 1.856, test_loss: 1.063\n",
            "epoch_nr: 27, train_loss: 1.897, test_loss: 1.079\n",
            "epoch_nr: 28, train_loss: 1.881, test_loss: 1.080\n",
            "epoch_nr: 29, train_loss: 1.845, test_loss: 1.072\n",
            "epoch_nr: 30, train_loss: 1.830, test_loss: 1.050\n",
            "epoch_nr: 31, train_loss: 1.845, test_loss: 1.052\n",
            "epoch_nr: 32, train_loss: 1.828, test_loss: 1.068\n",
            "epoch_nr: 33, train_loss: 1.827, test_loss: 1.039\n",
            "epoch_nr: 34, train_loss: 1.819, test_loss: 1.061\n",
            "epoch_nr: 35, train_loss: 1.811, test_loss: 1.050\n",
            "epoch_nr: 36, train_loss: 1.841, test_loss: 1.044\n",
            "epoch_nr: 37, train_loss: 1.840, test_loss: 1.035\n",
            "epoch_nr: 38, train_loss: 1.817, test_loss: 1.049\n",
            "epoch_nr: 39, train_loss: 1.827, test_loss: 1.038\n",
            "epoch_nr: 40, train_loss: 1.809, test_loss: 1.054\n",
            "epoch_nr: 41, train_loss: 1.795, test_loss: 1.079\n",
            "epoch_nr: 42, train_loss: 1.788, test_loss: 1.039\n",
            "epoch_nr: 43, train_loss: 1.792, test_loss: 1.046\n",
            "epoch_nr: 44, train_loss: 1.780, test_loss: 1.036\n",
            "epoch_nr: 45, train_loss: 1.818, test_loss: 1.047\n",
            "epoch_nr: 46, train_loss: 1.774, test_loss: 1.039\n",
            "epoch_nr: 47, train_loss: 1.771, test_loss: 1.056\n",
            "epoch_nr: 48, train_loss: 1.766, test_loss: 1.032\n",
            "epoch_nr: 49, train_loss: 1.750, test_loss: 1.029\n",
            "epoch_nr: 50, train_loss: 1.754, test_loss: 1.049\n",
            "epoch_nr: 51, train_loss: 1.756, test_loss: 1.028\n",
            "epoch_nr: 52, train_loss: 1.783, test_loss: 1.040\n",
            "epoch_nr: 53, train_loss: 1.753, test_loss: 1.026\n",
            "epoch_nr: 54, train_loss: 1.735, test_loss: 1.038\n",
            "epoch_nr: 55, train_loss: 1.742, test_loss: 1.030\n",
            "epoch_nr: 56, train_loss: 1.743, test_loss: 1.025\n",
            "epoch_nr: 57, train_loss: 1.755, test_loss: 1.033\n",
            "epoch_nr: 58, train_loss: 1.725, test_loss: 1.029\n",
            "epoch_nr: 59, train_loss: 1.742, test_loss: 1.058\n",
            "epoch_nr: 60, train_loss: 1.731, test_loss: 1.037\n",
            "epoch_nr: 61, train_loss: 1.723, test_loss: 1.028\n",
            "epoch_nr: 62, train_loss: 1.727, test_loss: 1.034\n",
            "epoch_nr: 63, train_loss: 1.743, test_loss: 1.028\n",
            "epoch_nr: 64, train_loss: 1.729, test_loss: 1.031\n",
            "epoch_nr: 65, train_loss: 1.728, test_loss: 1.037\n",
            "epoch_nr: 66, train_loss: 1.759, test_loss: 1.027\n",
            "epoch_nr: 67, train_loss: 1.721, test_loss: 1.043\n",
            "epoch_nr: 68, train_loss: 1.716, test_loss: 1.024\n",
            "epoch_nr: 69, train_loss: 1.728, test_loss: 1.060\n",
            "epoch_nr: 70, train_loss: 1.732, test_loss: 1.032\n",
            "epoch_nr: 71, train_loss: 1.709, test_loss: 1.049\n",
            "epoch_nr: 72, train_loss: 1.714, test_loss: 1.024\n",
            "epoch_nr: 73, train_loss: 1.729, test_loss: 1.028\n",
            "epoch_nr: 74, train_loss: 1.721, test_loss: 1.033\n",
            "epoch_nr: 75, train_loss: 1.707, test_loss: 1.027\n",
            "epoch_nr: 76, train_loss: 1.705, test_loss: 1.031\n",
            "epoch_nr: 77, train_loss: 1.716, test_loss: 1.031\n",
            "epoch_nr: 78, train_loss: 1.702, test_loss: 1.125\n",
            "epoch_nr: 79, train_loss: 1.704, test_loss: 1.029\n",
            "epoch_nr: 80, train_loss: 1.696, test_loss: 1.021\n",
            "epoch_nr: 81, train_loss: 1.725, test_loss: 1.021\n",
            "epoch_nr: 82, train_loss: 1.699, test_loss: 1.031\n",
            "epoch_nr: 83, train_loss: 1.698, test_loss: 1.069\n",
            "epoch_nr: 84, train_loss: 1.715, test_loss: 1.031\n",
            "epoch_nr: 85, train_loss: 1.682, test_loss: 1.020\n",
            "epoch_nr: 86, train_loss: 1.693, test_loss: 1.028\n",
            "epoch_nr: 87, train_loss: 1.694, test_loss: 1.033\n",
            "epoch_nr: 88, train_loss: 1.682, test_loss: 1.025\n",
            "epoch_nr: 89, train_loss: 1.687, test_loss: 1.053\n",
            "epoch_nr: 90, train_loss: 1.692, test_loss: 1.030\n",
            "epoch_nr: 91, train_loss: 1.686, test_loss: 1.030\n",
            "epoch_nr: 92, train_loss: 1.680, test_loss: 1.045\n",
            "epoch_nr: 93, train_loss: 1.696, test_loss: 1.040\n",
            "epoch_nr: 94, train_loss: 1.674, test_loss: 1.027\n",
            "epoch_nr: 95, train_loss: 1.676, test_loss: 1.029\n",
            "epoch_nr: 96, train_loss: 1.689, test_loss: 1.073\n",
            "epoch_nr: 97, train_loss: 1.679, test_loss: 1.025\n",
            "epoch_nr: 98, train_loss: 1.658, test_loss: 1.021\n",
            "epoch_nr: 99, train_loss: 1.687, test_loss: 1.045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiyNSZrPBUxN",
        "colab_type": "text"
      },
      "source": [
        "### 3. Add more hidden layers/neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgW00lWJMW4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ba650f6-f035-49cf-cc85-7d8b46199a5a"
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l1_reg': True,  # L1 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 228,  # Number of hidden neurons, add 100 more hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train_with_ReLU_L1(FLAGS)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 9.771, test_loss: 2.413\n",
            "epoch_nr: 1, train_loss: 3.018, test_loss: 2.167\n",
            "epoch_nr: 2, train_loss: 2.773, test_loss: 2.110\n",
            "epoch_nr: 3, train_loss: 2.640, test_loss: 2.005\n",
            "epoch_nr: 4, train_loss: 2.546, test_loss: 1.861\n",
            "epoch_nr: 5, train_loss: 2.482, test_loss: 1.806\n",
            "epoch_nr: 6, train_loss: 2.417, test_loss: 1.670\n",
            "epoch_nr: 7, train_loss: 2.357, test_loss: 1.569\n",
            "epoch_nr: 8, train_loss: 2.310, test_loss: 1.444\n",
            "epoch_nr: 9, train_loss: 2.264, test_loss: 1.354\n",
            "epoch_nr: 10, train_loss: 2.234, test_loss: 1.252\n",
            "epoch_nr: 11, train_loss: 2.212, test_loss: 1.233\n",
            "epoch_nr: 12, train_loss: 2.156, test_loss: 1.169\n",
            "epoch_nr: 13, train_loss: 2.147, test_loss: 1.161\n",
            "epoch_nr: 14, train_loss: 2.138, test_loss: 1.131\n",
            "epoch_nr: 15, train_loss: 2.137, test_loss: 1.181\n",
            "epoch_nr: 16, train_loss: 2.091, test_loss: 1.113\n",
            "epoch_nr: 17, train_loss: 2.046, test_loss: 1.132\n",
            "epoch_nr: 18, train_loss: 2.046, test_loss: 1.091\n",
            "epoch_nr: 19, train_loss: 2.045, test_loss: 1.119\n",
            "epoch_nr: 20, train_loss: 2.044, test_loss: 1.082\n",
            "epoch_nr: 21, train_loss: 2.058, test_loss: 1.100\n",
            "epoch_nr: 22, train_loss: 2.005, test_loss: 1.064\n",
            "epoch_nr: 23, train_loss: 2.004, test_loss: 1.055\n",
            "epoch_nr: 24, train_loss: 1.996, test_loss: 1.071\n",
            "epoch_nr: 25, train_loss: 1.975, test_loss: 1.063\n",
            "epoch_nr: 26, train_loss: 1.972, test_loss: 1.055\n",
            "epoch_nr: 27, train_loss: 1.966, test_loss: 1.066\n",
            "epoch_nr: 28, train_loss: 1.980, test_loss: 1.058\n",
            "epoch_nr: 29, train_loss: 1.954, test_loss: 1.052\n",
            "epoch_nr: 30, train_loss: 1.943, test_loss: 1.086\n",
            "epoch_nr: 31, train_loss: 1.934, test_loss: 1.055\n",
            "epoch_nr: 32, train_loss: 1.935, test_loss: 1.081\n",
            "epoch_nr: 33, train_loss: 1.933, test_loss: 1.047\n",
            "epoch_nr: 34, train_loss: 1.921, test_loss: 1.081\n",
            "epoch_nr: 35, train_loss: 1.905, test_loss: 1.040\n",
            "epoch_nr: 36, train_loss: 1.921, test_loss: 1.038\n",
            "epoch_nr: 37, train_loss: 1.894, test_loss: 1.064\n",
            "epoch_nr: 38, train_loss: 1.888, test_loss: 1.039\n",
            "epoch_nr: 39, train_loss: 1.896, test_loss: 1.033\n",
            "epoch_nr: 40, train_loss: 1.893, test_loss: 1.031\n",
            "epoch_nr: 41, train_loss: 1.894, test_loss: 1.057\n",
            "epoch_nr: 42, train_loss: 1.893, test_loss: 1.035\n",
            "epoch_nr: 43, train_loss: 1.880, test_loss: 1.052\n",
            "epoch_nr: 44, train_loss: 1.893, test_loss: 1.059\n",
            "epoch_nr: 45, train_loss: 1.863, test_loss: 1.028\n",
            "epoch_nr: 46, train_loss: 1.870, test_loss: 1.036\n",
            "epoch_nr: 47, train_loss: 1.883, test_loss: 1.032\n",
            "epoch_nr: 48, train_loss: 1.871, test_loss: 1.041\n",
            "epoch_nr: 49, train_loss: 1.855, test_loss: 1.031\n",
            "epoch_nr: 50, train_loss: 1.847, test_loss: 1.051\n",
            "epoch_nr: 51, train_loss: 1.865, test_loss: 1.043\n",
            "epoch_nr: 52, train_loss: 1.843, test_loss: 1.034\n",
            "epoch_nr: 53, train_loss: 1.858, test_loss: 1.062\n",
            "epoch_nr: 54, train_loss: 1.857, test_loss: 1.046\n",
            "epoch_nr: 55, train_loss: 1.850, test_loss: 1.030\n",
            "epoch_nr: 56, train_loss: 1.855, test_loss: 1.047\n",
            "epoch_nr: 57, train_loss: 1.858, test_loss: 1.024\n",
            "epoch_nr: 58, train_loss: 1.831, test_loss: 1.025\n",
            "epoch_nr: 59, train_loss: 1.850, test_loss: 1.046\n",
            "epoch_nr: 60, train_loss: 1.839, test_loss: 1.024\n",
            "epoch_nr: 61, train_loss: 1.848, test_loss: 1.032\n",
            "epoch_nr: 62, train_loss: 1.835, test_loss: 1.025\n",
            "epoch_nr: 63, train_loss: 1.835, test_loss: 1.019\n",
            "epoch_nr: 64, train_loss: 1.820, test_loss: 1.028\n",
            "epoch_nr: 65, train_loss: 1.836, test_loss: 1.031\n",
            "epoch_nr: 66, train_loss: 1.834, test_loss: 1.028\n",
            "epoch_nr: 67, train_loss: 1.828, test_loss: 1.028\n",
            "epoch_nr: 68, train_loss: 1.822, test_loss: 1.029\n",
            "epoch_nr: 69, train_loss: 1.819, test_loss: 1.026\n",
            "epoch_nr: 70, train_loss: 1.829, test_loss: 1.032\n",
            "epoch_nr: 71, train_loss: 1.819, test_loss: 1.024\n",
            "epoch_nr: 72, train_loss: 1.825, test_loss: 1.025\n",
            "epoch_nr: 73, train_loss: 1.836, test_loss: 1.029\n",
            "epoch_nr: 74, train_loss: 1.821, test_loss: 1.064\n",
            "epoch_nr: 75, train_loss: 1.831, test_loss: 1.035\n",
            "epoch_nr: 76, train_loss: 1.817, test_loss: 1.025\n",
            "epoch_nr: 77, train_loss: 1.822, test_loss: 1.026\n",
            "epoch_nr: 78, train_loss: 1.812, test_loss: 1.030\n",
            "epoch_nr: 79, train_loss: 1.816, test_loss: 1.026\n",
            "epoch_nr: 80, train_loss: 1.813, test_loss: 1.029\n",
            "epoch_nr: 81, train_loss: 1.809, test_loss: 1.017\n",
            "epoch_nr: 82, train_loss: 1.804, test_loss: 1.029\n",
            "epoch_nr: 83, train_loss: 1.810, test_loss: 1.037\n",
            "epoch_nr: 84, train_loss: 1.813, test_loss: 1.037\n",
            "epoch_nr: 85, train_loss: 1.802, test_loss: 1.040\n",
            "epoch_nr: 86, train_loss: 1.810, test_loss: 1.028\n",
            "epoch_nr: 87, train_loss: 1.817, test_loss: 1.040\n",
            "epoch_nr: 88, train_loss: 1.804, test_loss: 1.049\n",
            "epoch_nr: 89, train_loss: 1.817, test_loss: 1.022\n",
            "epoch_nr: 90, train_loss: 1.801, test_loss: 1.042\n",
            "epoch_nr: 91, train_loss: 1.800, test_loss: 1.026\n",
            "epoch_nr: 92, train_loss: 1.799, test_loss: 1.029\n",
            "epoch_nr: 93, train_loss: 1.796, test_loss: 1.022\n",
            "epoch_nr: 94, train_loss: 1.801, test_loss: 1.022\n",
            "epoch_nr: 95, train_loss: 1.814, test_loss: 1.047\n",
            "epoch_nr: 96, train_loss: 1.803, test_loss: 1.031\n",
            "epoch_nr: 97, train_loss: 1.798, test_loss: 1.032\n",
            "epoch_nr: 98, train_loss: 1.787, test_loss: 1.050\n",
            "epoch_nr: 99, train_loss: 1.787, test_loss: 1.037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKvFU1H6BkhV",
        "colab_type": "text"
      },
      "source": [
        "### 4. Drop some hidden neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vigu4pYMBkBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36ac4f23-0575-4351-86cb-f47656c88973"
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l1_reg': True,  # L1 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 78,  # Number of hidden neurons, drop 50 hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train_with_ReLU_L1(FLAGS)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_nr: 0, train_loss: 6.666, test_loss: 3.839\n",
            "epoch_nr: 1, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 2, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 3, train_loss: 3.820, test_loss: 3.839\n",
            "epoch_nr: 4, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 5, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 6, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 7, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 8, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 9, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 10, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 11, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 12, train_loss: 3.810, test_loss: 3.839\n",
            "epoch_nr: 13, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 14, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 15, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 16, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 17, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 18, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 19, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 20, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 21, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 22, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 23, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 24, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 25, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 26, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 27, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 28, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 29, train_loss: 3.812, test_loss: 3.839\n",
            "epoch_nr: 30, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 31, train_loss: 3.811, test_loss: 3.839\n",
            "epoch_nr: 32, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 33, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 34, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 35, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 36, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 37, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 38, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 39, train_loss: 3.819, test_loss: 3.839\n",
            "epoch_nr: 40, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 41, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 42, train_loss: 3.812, test_loss: 3.839\n",
            "epoch_nr: 43, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 44, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 45, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 46, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 47, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 48, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 49, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 50, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 51, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 52, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 53, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 54, train_loss: 3.820, test_loss: 3.839\n",
            "epoch_nr: 55, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 56, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 57, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 58, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 59, train_loss: 3.812, test_loss: 3.839\n",
            "epoch_nr: 60, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 61, train_loss: 3.822, test_loss: 3.839\n",
            "epoch_nr: 62, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 63, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 64, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 65, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 66, train_loss: 3.817, test_loss: 3.839\n",
            "epoch_nr: 67, train_loss: 3.819, test_loss: 3.839\n",
            "epoch_nr: 68, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 69, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 70, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 71, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 72, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 73, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 74, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 75, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 76, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 77, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 78, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 79, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 80, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 81, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 82, train_loss: 3.820, test_loss: 3.839\n",
            "epoch_nr: 83, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 84, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 85, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 86, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 87, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 88, train_loss: 3.812, test_loss: 3.839\n",
            "epoch_nr: 89, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 90, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 91, train_loss: 3.815, test_loss: 3.839\n",
            "epoch_nr: 92, train_loss: 3.816, test_loss: 3.839\n",
            "epoch_nr: 93, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 94, train_loss: 3.814, test_loss: 3.839\n",
            "epoch_nr: 95, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 96, train_loss: 3.813, test_loss: 3.839\n",
            "epoch_nr: 97, train_loss: 3.811, test_loss: 3.839\n",
            "epoch_nr: 98, train_loss: 3.818, test_loss: 3.839\n",
            "epoch_nr: 99, train_loss: 3.816, test_loss: 3.839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rml4fuhPL6qE",
        "colab_type": "text"
      },
      "source": [
        "### 5. Batch Normalization (BN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGgyTPbFL4b5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import model_helper\n",
        "\n",
        "\n",
        "def _get_bias_initializer():\n",
        "    return tf.zeros_initializer()\n",
        "\n",
        "\n",
        "def _get_weight_initializer():\n",
        "    return tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
        "\n",
        "\n",
        "class DAE_ReLU_L1: #Data Acquisition Engine\n",
        "    \n",
        "    def __init__(self, FLAGS):\n",
        "        ''' Implementation of deep autoencoder class.'''\n",
        "        \n",
        "        self.FLAGS = FLAGS\n",
        "        self.weight_initializer = _get_weight_initializer()\n",
        "        self.bias_initializer = _get_bias_initializer()\n",
        "        self.init_parameters()\n",
        "        \n",
        "\n",
        "    def init_parameters(self):\n",
        "        '''Initialize networks weights and biasis.'''\n",
        "        \n",
        "        with tf.name_scope('weights'):\n",
        "          #This context manager validates that the given values are from the same graph,  \n",
        "          #makes that graph the default graph, and pushes a name scope in that graph\n",
        "            self.W_1 = tf.get_variable(name='weight_1', shape=(self.FLAGS['num_v'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer) #Gets an existing variable with these parameters or create a new one\n",
        "            self.W_2 = tf.get_variable(name='weight_2', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_3 = tf.get_variable(name='weight_3', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_4 = tf.get_variable(name='weight_4', shape=(self.FLAGS['num_h'], self.FLAGS['num_v']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "        \n",
        "        with tf.name_scope('biases'):\n",
        "            self.b1 = tf.get_variable(name='bias_1', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b2 = tf.get_variable(name='bias_2', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b3 = tf.get_variable(name='bias_3', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "        \n",
        "    def _inference(self, x):\n",
        "        ''' Making one forward pass. Predicting the networks outputs.\n",
        "        @param x: input ratings\n",
        "        \n",
        "        @return : networks predictions\n",
        "        '''\\\n",
        "        \n",
        "        with tf.name_scope('inference'):\n",
        "             a1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, self.W_1),self.b1)) # sign(W1T*X+b1)\n",
        "             a2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a1, self.W_2),self.b2))\n",
        "             a3 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a2, self.W_3),self.b3))\n",
        "             a4 = tf.matmul(a3, self.W_4)\n",
        "        return a4\n",
        "    \n",
        "    def _compute_loss(self, predictions, labels, num_labels):\n",
        "        ''' Computing the Mean Squared Error loss between the input and output of the network.\n",
        "            \n",
        "          @param predictions: predictions of the stacked autoencoder\n",
        "          @param labels: input values of the stacked autoencoder which serve as labels at the same time\n",
        "          @param num_labels: number of labels !=0 in the data set to compute the mean\n",
        "            \n",
        "          @return mean squared error loss tf-operation\n",
        "          '''\n",
        "            \n",
        "        with tf.name_scope('loss'):\n",
        "            loss_op = tf.div(tf.reduce_sum(tf.square(tf.subtract(predictions,labels))),num_labels)\n",
        "            return loss_op\n",
        "          \n",
        "    def _l1_loss(self, params):\n",
        "        '''\n",
        "        Unfortunately, L1 isn’t a tensorflow function, so we have to create it by ourselves and use it instead of L2\n",
        "        '''\n",
        "        return tf.reduce_sum(tf.abs(params))\n",
        "\n",
        "    def _optimizer(self, x):\n",
        "        '''Optimization of the network parameter through stochastic gradient descent.\n",
        "            \n",
        "            @param x: input values for the stacked autoencoder.\n",
        "            \n",
        "            @return: tensorflow training operation\n",
        "            @return: ROOT!! mean squared error\n",
        "        '''\n",
        "        \n",
        "        outputs = self._inference(x)\n",
        "        # ? the mask is same as x ?\n",
        "        mask = tf.where(tf.equal(x,0.0), tf.zeros_like(x), x) # indices of 0 values in the training set\n",
        "        # tf.zero_like : Creates a tensor with all elements set to zero.\n",
        "        # The condition tensor acts as a mask that chooses, based on the value at each element, \n",
        "        # whether the corresponding element / row in the output should be taken from x (if true) or y (if false).\n",
        "        num_train_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # number of non zero values in the training set\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) # boolean mask\n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs)) # set the output values to zero if corresponding input values are zero\n",
        "        # ? why set output values to zero? does'nt it change the output?\n",
        "\n",
        "        MSE_loss = self._compute_loss(outputs,x,num_train_labels)\n",
        "        \n",
        "        if self.FLAGS['l1_reg'] == True:\n",
        "            l1_loss = tf.add_n([self._l1_loss(v) for v in tf.trainable_variables()]) # Returns all variables created with trainable=True.\n",
        "            MSE_loss = MSE_loss +  self.FLAGS['lambda_'] * l1_loss\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.FLAGS['learning_rate']).minimize(MSE_loss) #An Operation that updates the variables in var_list\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "\n",
        "        return train_op, RMSE_loss\n",
        "    \n",
        "    def _validation_loss(self, x_train, x_test):\n",
        "        ''' Computing the loss during the validation time.\n",
        "            \n",
        "          @param x_train: training data samples\n",
        "          @param x_test: test data samples\n",
        "            \n",
        "          @return networks predictions\n",
        "          @return root mean squared error loss between the predicted and actual ratings\n",
        "          '''\n",
        "        \n",
        "        outputs = self._inference(x_train) # use training sample to make prediction\n",
        "        mask = tf.where(tf.equal(x_test,0.0), tf.zeros_like(x_test), x_test) # identify the zero values in the test ste\n",
        "        num_test_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # count the number of non zero values\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) \n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs))\n",
        "    \n",
        "        MSE_loss = self._compute_loss(outputs, x_test, num_test_labels)\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "            \n",
        "        return outputs, RMSE_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA3dQTrMYoNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGdIXRNFYoAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8mIr8KZnLt8",
        "colab_type": "text"
      },
      "source": [
        "## Technical Analysis\n",
        "\n",
        "1)  In this project, we accessed the ml-1m.zip movie rating dataset downloadable from https://grouplens.org/datasets/movielens, then mounted and copied the file to a Google Colab environment. The goal of this project is to build a encoder-decoder deep learning model for movie recommendations.\n",
        "\n",
        "2)  **Followed Steps Below:**\n",
        "1.  Define the network structure\n",
        "2. Activation function: Sigmoid, ReLu\n",
        "3. Define the cost function\n",
        "4. RMSE - model evaluation metric\n",
        "5. Optimizer - apply L1/L2 regularization terms\n",
        "6. Define the input data \n",
        "7. Extract data from input \n",
        "\n",
        "\n",
        "\n",
        "3)  **Key Observations:**\n",
        "*   Model is overfitting without applying L1/L2 regularization terms, the ReLu case: with low training errors and high testing errors. \n",
        "*   Unfortunately, L1 isn’t a tensorflow function, so we have to create it by ourselves and use it instead of L2. As a result of it, we pre-defined _11_loss function that takes in a set of parameters.\n",
        "*   Overall, Sigmoid function outputs better results compared to ReLu activation function. \n",
        "*   However, ReLu with L1 regularization outputs minimum testing error after 100 epochs.\n",
        "*   We evaluated model performance by looking at both training errors and testing errors. In conclusion, Sigmoid with no regularization term and ReLu with L1 seem more appropriate for this case. More generalized for new data.\n",
        "*  Adding more hidden neurons does not significantly decrease errors, neither on training errors nor on testing errors. Poor model performance especially for training data. Whereas fewer layers result in a high RMSE scenario. In this case, model seems to be underfitting.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTeKO8L-mq3u",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Regularization with TensorFlow: http://www.godeep.ml/regularization-using-tensorflow\n",
        "\n",
        "Why are deep neural networks hard to train? http://neuralnetworksanddeeplearning.com/chap5.html"
      ]
    }
  ]
}